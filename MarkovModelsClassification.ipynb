{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/spyros/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/spyros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1                @VirginAmerica What @dhepburn said.\n",
       "1          0  @VirginAmerica plus you've added commercials t...\n",
       "2          1  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          2  @VirginAmerica it's really aggressive to blast...\n",
       "4          2  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2    9178\n",
       "1    3099\n",
       "0    2363\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv', index_col=None)\n",
    "#df = df.loc[:8000,:]\n",
    "cols2Keep=['airline_sentiment', 'text']\n",
    "df = df[cols2Keep]\n",
    "df.rename(columns={'airline_sentiment':'sentiment'}, inplace=True)\n",
    "df.replace({'neutral':1, 'positive':0, 'negative':2}, inplace=True)\n",
    "display(df.shape, df.head(), df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>“@JetBlue: @FinleyBklynCFS So glad to hear. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways that's why u guys are my #1 choice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Haha I had a boarding pass for 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>@USAirways if I try and call your reservations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@united @jsumiyasu I am thankful to the  Unite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  “@JetBlue: @FinleyBklynCFS So glad to hear. Th...\n",
       "1          0     @USAirways that's why u guys are my #1 choice.\n",
       "2          0  @AmericanAir Haha I had a boarding pass for 12...\n",
       "3          2  @USAirways if I try and call your reservations...\n",
       "4          0  @united @jsumiyasu I am thankful to the  Unite..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    3099\n",
       "2    2938\n",
       "0    2363\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# balance the classes\n",
    "df = df.sort_values(by='sentiment')\n",
    "df = df.iloc[:8400,:]\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "display(df.shape, df.head(), df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for this problem. I should not remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# Expand the stopword list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "# Same goes to the punctuation list\n",
    "string.punctuation = string.punctuation + '£' +'’' + '...'\n",
    "\n",
    "    \n",
    "# Function to clean the text\n",
    "def cleaningText(text, stem, lemm):\n",
    "    \n",
    "    text = text.lower() # Lowercase \n",
    "    text = text.strip() # Remove leading/trailing whitespace\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text) # Remove punctuation\n",
    "    text = re.sub('\\s+', ' ', text) # Remove extra space and tabs\n",
    "    text = re.compile('<.*?>').sub('', text) # Remove any HTML tags/markups\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    filtered_sentence=[]\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    text = \" \".join(filtered_sentence)\n",
    "    \n",
    "    if stem:\n",
    "        # Initialize the stemmer\n",
    "        snow = SnowballStemmer('english')\n",
    "\n",
    "        stemmed_sentence = []\n",
    "        # Tokenize the sentence\n",
    "        words = word_tokenize(text)\n",
    "        for w in words:\n",
    "            # Stem the word/token\n",
    "            stemmed_sentence.append(snow.stem(w))\n",
    "        text = \" \".join(stemmed_sentence)\n",
    "    \n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # This is a helper function to map NTLK position tags\n",
    "    def get_wordnet_position(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    if lemm:\n",
    "        lemmatized_sentence = []\n",
    "        # Tokenize the sentence\n",
    "        words = word_tokenize(text)\n",
    "        # Get position tags\n",
    "        word_pos_tags = nltk.pos_tag(words)\n",
    "        # Map the position tag and lemmatize the word/token\n",
    "        for idx, tag in enumerate(word_pos_tags):\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(tag[0], get_wordnet_position(tag[1])))\n",
    "        text = \" \".join(lemmatized_sentence)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# I will not use stem - found out that it removes the suffixes in a way that is not helpful here. Lemmatizer is doing a better job at this case.\n",
    "df['textClean'] = df.apply(lambda x: cleaningText(x['text'], False, False),  axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the data\n",
    "input_texts = df['textClean'].values\n",
    "labels = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7980, 420)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split my data\n",
    "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts, labels, test_size=0.05)\n",
    "len(Ytrain), len(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "word2index = {'<unknown>': 0}\n",
    "\n",
    "# populate word2idx\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token not in word2index:\n",
    "            word2index[token] = counter\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10768\n"
     ]
    }
   ],
   "source": [
    "voc_len = len(word2index)\n",
    "print(voc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into integer format\n",
    "integerTrainText = []\n",
    "integerTestText = []\n",
    "\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    integerSentence = [word2index[token] for token in tokens]  #assign the unique integer of every word in the sentence to the corresponding list\n",
    "    integerTrainText.append(integerSentence)\n",
    "\n",
    "for text in test_text:\n",
    "    tokens = text.split()\n",
    "    integerSentence = [word2index.get(token, 0) for token in tokens]\n",
    "    integerTestText.append(integerSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize A and pi matrices - for both sentiments\n",
    "V = len(word2index)\n",
    "\n",
    "A0 = np.ones((V, V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "A1 = np.ones((V, V))\n",
    "pi1 = np.ones(V)\n",
    "\n",
    "A2 = np.ones((V, V))\n",
    "pi2 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute counts for A and pi\n",
    "# This is no other than the fitting part of any sklearn interface approximation\n",
    "def compute_counts(integerText, A, pi):\n",
    "    for tokens in integerText:\n",
    "        last_idx = None\n",
    "        for idx in tokens:\n",
    "            if last_idx is None:\n",
    "            # it's the first word in a sentence, so we need to populate pi arrays for both classes\n",
    "                pi[idx] += 1\n",
    "            else:\n",
    "                # the last word exists, so count a transition. The transition matrix is the A matrix which is \n",
    "                # populated by previous word transiting to the next word\n",
    "                A[last_idx, idx] += 1\n",
    "\n",
    "            # update last idx to correct its value for the next iterration of the for block\n",
    "            last_idx = idx\n",
    "\n",
    "\n",
    "compute_counts([t for t, y in zip(integerTrainText, Ytrain) if y == 0], A0, pi0)   #input parameters ~ keep only zipped of sentence integers with the class\n",
    "compute_counts([t for t, y in zip(integerTrainText, Ytrain) if y == 1], A1, pi1)   #will update my A, pi matrices for each class\n",
    "compute_counts([t for t, y in zip(integerTrainText, Ytrain) if y == 2], A2, pi2)   #will update my A, pi matrices for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize A and pi so they are converted from counts to valid probability matrices\n",
    "\n",
    "A0 /= A0.sum(axis=1, keepdims=True)   # keepdims to format 2D products\n",
    "pi0 /= pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /= pi1.sum()\n",
    "\n",
    "A2 /= A2.sum(axis=1, keepdims=True)\n",
    "pi2 /= pi2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with log probabilities to avoid small multiplications effects\n",
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)\n",
    "\n",
    "logA2 = np.log(A2)\n",
    "logpi2 = np.log(pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2794486215538847, 0.3705513784461153, 0.35)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prior probabilities computation\n",
    "count0 = sum(y == 0 for y in Ytrain)\n",
    "count1 = sum(y == 1 for y in Ytrain)\n",
    "count2 = sum(y == 2 for y in Ytrain)\n",
    "total = len(Ytrain)\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "p2 = count2 / total\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)\n",
    "logp2 = np.log(p2)\n",
    "p0, p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "class Classifier:\n",
    "    def __init__(self, logAs, logpis, logpriors): #the constructor will take as inputs the log form of transition matrix, the initial pis and the priors probabilities\n",
    "        self.logAs = logAs\n",
    "        self.logpis = logpis\n",
    "        self.logpriors = logpriors\n",
    "        self.K = len(logpriors) # number of classes\n",
    "\n",
    "    def _compute_log_likelihood(self, input_, class_): #parameters: input text of integers with the corresponding class\n",
    "        logA = self.logAs[class_]                    #the class value will determine which of 2 Markov Model is to be used\n",
    "        logpi = self.logpis[class_]\n",
    "\n",
    "        last_idx = None\n",
    "        logprob = 0   #initialize the final answer\n",
    "        for idx in input_:\n",
    "            if last_idx is None:\n",
    "            # it's the first token of the sentence so we use pi matrix\n",
    "                logprob += logpi[idx]\n",
    "            else:\n",
    "                # it's NOT the first token of the sentence so we use the state transition matrix\n",
    "                logprob += logA[last_idx, idx]\n",
    "      \n",
    "            # update last_idx for the next iterration\n",
    "            last_idx = idx\n",
    "    \n",
    "        return logprob\n",
    "  \n",
    "    def predict(self, inputs):\n",
    "        predictions = np.zeros(len(inputs)) #store the preds\n",
    "        for i, input_ in enumerate(inputs):\n",
    "            posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] for c in range(self.K)]\n",
    "            pred = np.argmax(posteriors)  #get the largest probability as a presult\n",
    "            predictions[i] = pred #store it\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array must be in order since classes are assumed to index these lists\n",
    "clf = Classifier([logA0, logA1, logA2], [logpi0, logpi1, logpi2], [logp0, logp1, logp2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.966\n"
     ]
    }
   ],
   "source": [
    "Ptrain = clf.predict(integerTrainText)\n",
    "print(f\"Train acc: {round(np.mean(Ptrain == Ytrain),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.707\n"
     ]
    }
   ],
   "source": [
    "Ptest = clf.predict(integerTestText)\n",
    "print(f\"Test acc: {round(np.mean(Ptest == Ytest),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
